SnapRAID TODO
=============

This is the list of TODO items for SnapRAID.

- v4.0

* In the content file, for each position in the parity store the last time
the hash/parity was checked for all the blocks with sync.
We can share the same time info for all the blocks at a given position.
It will allow to define a new "scrub" command, checking only aged blocks
and skipping recently synched ones.
The "scrub" command won't change data in the disks, but it will update the content
file.
The check command should be maintaned independent, and not saving the content file.
The format could be:
tim BLOCK TIME
tir FROM_BLOCK TO_BLOCK TIME
For missing block, we can use 0 as initialization value. TIME is the UNIX time.

- v4.x

* In the import list, uses also all the blocks in the array.
But we must cover the case of bad blocks. Likely we can just check the
hash after reading, and in case, skip it, and retry with another copy.

* If a content file is missing, sync should rewrite it, even if no change was done at the content.

* A new 'init' command to differentiate the first 'sync' operation.
This 'init' will work also without a content file, and parity files.
Instead 'sync' will require all of them.
This will also help when running with the parity filesystem unmounted.

* Adds a log configuration option to save log files with the date in the name.
This could help to keep track of what happens during automated operations.

* In fix an existing symlink with the same name of a file to be recovered may stop
the process making the create() operation to fail.
The same for directories, when recreating the directory tree.

* If a directory exists with the same name of a parity/content file be more explicative
on the error message. See: https://sourceforge.net/projects/snapraid/forums/forum/1677233/topic/4861034

* Support interrupted operations with EINTR. For remote filesystems it could be relevant.

* Allow to specify more than one disk directories to cover the case of multi partitions.
Different partitions have duplicate inode. The only way to support this is to
add also a kind of device_id, increasing the memory required.
But it should be only few bits for each file. So, it should be manageable.

* Save the content file in compressed .gz format to save space.

* Rename sync->backup and fix->restore. It seems to me a naming expressing
better the meaning of the commands. But not yet sure.

* We don't try to do partial block recovering. A block is correct or not.
But if only some bytes, or a sector, is wrong, it should be possible to recover all the
rest of the block.
The problem is that we don't have any hash to ensure that the block is partially recovered,
or completely garbage. But it makes sense to anyway write the "most likely" correct one.

* In the repair() function the euristic to detect if we recovered after the sync, can be extended
to all the previous blocks, because we always proceeed in block order during a sync.
So, if for a block we can detect that we recovered using updated parity data,
also for all the previous blocks this is true.
Anyway, the case where this information could be useful should be present
only if changes are committed after an aborted sync.

* When fixing, before overwriting the present file, make a copy of it just in
case that the original file cannot be completely recovered.
We can always open files in read-only mode, if a write is required, we close it,
rename it to with a .bak extension, and rewrite it up to the required size.
The same for symlink if a file with the same name exist or viceversa.

- v4.x pooling

* Allow to specify the shared path to use to access remote symlink.

* Add a new "commit" command to move changes from the pool to the array.
It should:
- Move files copied into the pool (that are no links) to the array.
The files should be moved to the disk that contains most of the files
in the directory. If no space, try with the disk with less files
in the directory, and eventually the disk in the array with more free space.
- Detect renames, and apply them in the array.
The file will be renamed and moved to the new directory, if changed,
but kept in the same disk of the array.
- Detect deletes, and move file in the array to a "/trash/" directory
of the same disk. For safety no real deletion is done.
File with the same name will get an extra extension like ".1", ".2".

- v5.x

* Triple parity also called RAID-TP. We can implement it in a compatible way
with the existing RAID-5 and RAID-6 definining a new r-parity file.
See http://blogs.oracle.com/ahl/entry/triple_parity_raid_z and
http://thread.gmane.org/gmane.linux.raid/34195/focus=34236

* The data could be compressed before processing, resulting in parity block of fixed size,
but matching different data block sizes.
The complexity is that a file blocks will have to be allocated at runtime,
and you may run out of them in the middle of the processing.
We need also a way to compress a stream until the compressed data reach the
block size, but no more, and then start a new block.
For each block, we'll have also to store. "size_uncompressed", "size_compressed",
"hash".

* In the content file save the timestap of the parity files.
If they do not match, stop the processing.
This can be done to avoid to use not syncronized parity and content files,
resulting in wrong data.
But if the sync process is killed we need a way to resyncronize them.
Or maybe we should allow parity never than content, but not viceversa.


Rejected TODO
=============

This is a list of rejected TODO items.

* Recognizes that a file is moved from one disk to another, and if the parity
data doesn't overlap, do not recompute it.
- It's going to work only in RAID5 mode and only in special cases.

* Implements a multithread sync command.
- At now it's questionable if it will result in a performance improvment.
The murmur3 hash, and the RAID5/6 computations are so fast that even a single
thread should be able to do them.
Use the "snapraid -T" comment to see the speed.
Also, all the file operations are already done in background by the OS,
so no improvement is expect from this side.

* The SSE2 RAID6 computation may be slower than the MMX version.
See for example this case on a AMD Zacate APU E-350 1.6GHz:
memset 1812 [MB/s]
MD5 252 [MB/s]
Murmur3 1116 [MB/s]
RAID5 int32x2 1692 [MB/s]
RAID5 mmxx2 1745 [MB/s]
RAID5 sse2x2 2481 [MB/s]
RAID6 int32x2 807 [MB/s]
RAID6 mmxx2 1304 [MB/s]
RAID6 sse2x2 787 [MB/s]
- It happens in some systems. At now not an high priority issue.

